Crawler project README.md

Xinchang Meng: 
1. First, the program establishes a TCP connection with the web server and sends an HTTP GET request for the login page. The program receives the HTTP response from the server and extracts the necessary cookies and tokens required for authentication. The program then sends an HTTP POST request to the server, including the necessary credentials and tokens to authenticate the user. The server responds with an HTTP response, indicating whether the login was successful or not. If successful, the program extracts the session cookies and uses them to navigate to the home page. The program then starts crawling the website by sending HTTP GET requests to URLs and parsing the HTML content of each page. As the program crawls the website, it searches for secret flags embedded within the HTML content of each page using the FakebookHTMLParser(). If the program encounters a URL that requires authentication, it will repeat the authentication process as described above. If the program encounters a URL that is a redirect, it will follow the redirect and continue crawling from the new URL. If the program encounters a URL that returns a 4xx status code, it will skip the URL and continue crawling. If the program encounters a URL that returns a 5xx status code, it will add the URL back to the queue and continue crawling. Once the program has found all five secret flags, it will stop crawling and print out the flags.
2. 1.Handling authentication errors: The code assumes that the authentication process will always succeed. However, if the server changes the authentication mechanism or requires additional parameters, the code may not be able to log in properly. 2.Parsing complex HTML: The FakebookHTMLParser class assumes that the HTML content it receives is well-formed and structured. If the HTML is malformed or contains unexpected elements, the parser may not work as expected.
3. I did test manually, involved navigating the website manually to verify that the crawler is properly following links, extracting content, and handling errors. This was time-consuming, but it helped me to uncover issues that may not be apparent from automated testing alone.
4. Finished Start crawling function. The function implements the basic web crawler by parsing through the HTML content of the current URL and searching for more URLs and/or secret flags until all secret flags are found for the user. The function also accounts for and appropriately handles different errors received when parsing through pages, such as redirect pages, response codes in the 4xx (client errors) and 5xx (server errors) ranges, and the cases where the response code is in the 2xx (success) range by using proper helper functions.
